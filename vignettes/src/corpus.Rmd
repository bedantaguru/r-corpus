<!--
%\VignetteIndexEntry{Introduction to corpus}
%\VignetteEngine{knitr::rmarkdown}
%\VignetteEncoding{UTF-8}
-->

Introduction to corpus
======================

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment = "", fig.path = "corpus-", fig.retina = TRUE)
options(width = 100)
palette(RColorBrewer::brewer.pal(6, "Set1"))
```

Overview
--------

Data preparation
----------------

For a demonstration text we will use L. Frank Baum's *The Wonderful Wizard of
Oz*, available as [Project Gutenberg EBook #55][gutenberg-oz]. We first
download the text and strip off the Project Gutenberg header and footer.

```{r}
url <- "http://www.gutenberg.org/cache/epub/55/pg55.txt"
raw <- readLines(url, encoding = "UTF-8")

# the text starts after the Project Gutenberg header...
start <- grep("^\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK", raw) + 1

# ...end ends at the Project Gutenberg footer.
stop <- grep("^End of Project Gutenberg", raw) - 1

lines <- raw[start:stop]
```

The novel starts with front matter: a title page, table of contents,
introduction, and half title page. Then, a series of chapters follow.
We group the lines by section.
```{r}
# the front matter ends at the half title page
half_title <- grep("^THE WONDERFUL WIZARD OF OZ", lines)

# chapters start with "1.", "2.", etc...
chapter <- grep("^[[:space:]]*[[:digit:]]+\\.", lines)

# ... and appear after the half title page
chapter <- chapter[chapter > half_title]

# get the section texts (including the front matter)
start <- c(1, chapter + 1)
end <- c(chapter - 1, length(lines))
text <- mapply(function(s, e) paste(lines[s:e], collapse = "\n"), start, end)

# discard the front matter
text <- text[-1]

# get the section titles, removing the prefix ("1.", "2.", etc.)
title <- sub("^[[:space:]]*[[:digit:]]+[.][[:space:]]*", "", lines[chapter])
```


Corpus object
-------------

Now that we have obtained our raw data, we put everything together into a
corpus object, constructed via the `corpus` function:

```{r}
data <- corpus(title, text)
rownames(data) <- sprintf("ch%02d", seq_along(chapter))
```

The `corpus` function behaves similarly to the `data.frame` function, but
expects one of the columns to be named `"text"`. Note that we do not need
to specify `stringsAsFactors = FALSE` when creating a corpus object.
As an alternative to using the `corpus` function, we can construct a data
frame using some other method (e.g., `read.csv` or `read_ndjson`) and use
the `as_corpus` function.


A corpus object is just a data frame with a column named "text" of type
`"corpus_text"`. When using the *corpus* library, it is not strictly necessary
to use corpus objects as inputs; most functions will accept with character
vectors and ordinary data frames. Using a corpus object gives better
printing behavior and allows setting a `text_filter` attribute to override
the default text preprocessing.


The `corpus` function returns a data frame with the class attribute set to
`c("corpus_frame", "data.frame")`, to make printing nicer:

```{r}
emoji <- data.frame(text = sapply(0x1f600 + 1:30, intToUtf8),
                    stringsAsFactors = FALSE)
print(emoji) # as a data frame
print(as_corpus(emoji)) # as a corpus (cuts off after 18 rows)
print(as_corpus(emoji), 5) # cuts off after 5 rows
```


Tokenization
------------

Text in *corpus* is represented as a sequence of tokens, each taking a value
in a set of types. We can see the tokens for one or more elements using
the `text_tokens` function:

```{r}
text_tokens(data["ch24",]) # Chapter 24's tokens
```

The default behavior is to normalize tokens by changing the cases of the
letters to lower case. A `text_filter` object controls the rules for
segmentation and normalization. We can inspect the text filter:
```{r}
text_filter(data)
```
If the text column is of type `corpus_text` as returned by `as_text`, then
we can change the text filter properties:
```{r}
text_filter(data)$map_case <- FALSE
text_filter(data)$drop_punct <- TRUE
text_tokens(data["ch24",])
```

To restore the defaults, set the text filter to `NULL`:
```{r}
text_filter(data) <- NULL
```
In addition to mapping case and quotes (the defaults), I'm going to drop
punctuation.
```{r}
text_filter(data) <- text_filter(drop_punct = TRUE)
```

The tokenizer allows for precise controlling over token dropping and token
stemming. It also allows combining two or more words into a single token as
in the following example:
```{r}
text_tokens("I live in New York City, New York",
            text_filter(combine = c("new york", "new york city")))
```
This example using the optional second argument to `text_tokens` to override
the first argument's default text filter.  Here, instances of "new york" and
"new york city" get replaced by single tokens, with the longest match taking
precedence. See the documentation for `text_tokens` describes the full
tokenization process.


Text statistics
---------------

The `text_ntoken`, `text_ntype`, and `text_nsentence` functions return the
numbers of tokens, unique types, and sentences, respectively, in a set of
texts. We can use these functions to get an overview of the section lengths
and lexical diversities.

```{r}
(stats <- text_stats(data))
```

[Heaps' law][heaps-law] says that the logarithm of the number of unique
types is a linear function of the number of tokens. We can test this law
formally with a regression analysis.

In this analysis, we will exclude the last chapter (Chapter 24), because it is
much shorter than the others and has a disproportionate influence on the fit.

```{r}
subset <- row.names(stats) != "ch24"
model <- lm(log(types) ~ log(tokens), stats, subset)
summary(model)
```

We can also inspect the relation visually
```{r heapslaw, fig.width = 12, fig.cap = "Heaps' Law"}
par(mfrow = c(1, 2))
plot(log(types) ~ log(tokens), stats, col = 2, subset = subset)
abline(model, col = 1, lty = 2)

plot(log(stats$tokens[subset]), rstandard(model), col = 2,
     xlab = "log(tokens)")
abline(h = 0, col = 1, lty = 2)

outlier <- abs(rstandard(model)) > 2
text(log(stats$tokens)[subset][outlier], rstandard(model)[outlier],
     row.names(stats)[subset][outlier], cex = 0.75, adj = c(-0.25, 0.5),
     col = 2)
```

The analysis tells us that Heap's law accurately characterizes the lexical
diversity (type-to-token ratio) for the main chapters in *The Wizard of Oz*.
The number of unique types grows roughly as the number of tokens raised to the
power `0.6`.


The one chapter with an unusually low lexical diversity is Chapter 16. This
chapter contains mostly dialogue between Oz and Dorothy's simple-minded
companions (the Scarecrow, Tin Woodman, and Lion).


Emotional affect
----------------

### Emotion lexicon

Next we will look at usage rates for emotion words from the WordNet Affect
lexicon. As a starting point, we will take the "Positive", "Negative", and
"Ambigous" emotion terms (exluding the other category, "Neutral").

```{r}
affect <- subset(wnaffect, emotion != "Neutral")
affect$emotion <- droplevels(affect$emotion) # drop the unused 'Neutral' level
affect$category <- droplevels(affect$category) # drop unused categories
```

Rather than blindly applying the lexicon, we first check to see what the most
common emotion terms are.
```{r}
counts <- term_stats(data)
subset(counts, term %in% affect$term)
```

A few terms jump out as unusual: "yellow" is probably for the yellow brick
road, and "wicked" is probably for the wicked witch. When these terms appear,
they probably don't describe an emotional state. We can verify this using the
`text_locate` function, which shows these terms in context.

```{r}
text_locate(data, "yellow")
text_locate(data, "wicked")
```

We can also inspect the first token after each appearance of "wicked":
```{r}
term_stats(text_sub(text_locate(data, "wicked")$after, 1, 1))
```
of the 72 appearances of "wicked", 58 are followed by "witch" or "witches".
Likewise, "yellow" is often referring to the road, or referring to the color
of an object and not an emotion:
```{r}
term_stats(text_sub(text_locate(data, "yellow")$after, 1, 1))
```

The word "heart" is also suspiciously frequent. Here are some occurrences of
that word:
```{r}
text_locate(data, "heart")
```

A central part of the novel's plot is the Tin Woodman's quest for a heart; it
is not surprising that the word "heart" shows up so frequently. Indeed, most
appearances of the word "heart" are within 25 tokens of "woodman":

```{r}
loc <- text_locate(data, "heart")
before <- text_detect(text_sub(loc$before, -25, -1), "woodman")
after <- text_detect(text_sub(loc$after, 1, 25), "woodman")
summary(before | after)
```
"Woodman" appears within 25 tokens of "heart" in in 45 of the 67
contexts where the latter word appears.


All of this analysis shows that we should probably exclude these terms if we
are interested in words that connote emotion in _The Wizard of Oz_:

```{r}
affect <- subset(affect, !term %in% c("heart", "wicked", "yellow"))
```

### Counting emotion words

```{r emotion, fig.width = 12, fig.cap = "Emotion in Oz"}

term_scores <- with(affect, unclass(table(term, emotion)))
term_scores <- term_scores / rowSums(term_scores)

chunks <- text_split(data, "tokens", 500)
n <- text_ntoken(chunks)
x <- term_matrix(chunks, select = rownames(term_scores))
text_scores <- x %*% term_scores

# compute the rates per 1000 tokens
unit <- 1000
rate <- list(pos = text_scores[, "Positive"] / n * unit,
             neg = text_scores[, "Negative"] / n * unit,
             ambig = text_scores[, "Ambiguous"] / n * unit)
rate$total <- rate$pos + rate$neg + rate$ambig

# compute the standard errors
se <- lapply(rate, function(r) sqrt(r * (unit - r) / n))

# set up segment IDs
i <- seq_len(nrow(chunks))

# set the plot margins, with extra space below the plot
par(mar = c(4, 4, 8, 6) + 0.1, las = 1)

# set up the plot coordinates; put labels but no axes
xlim <- range(i - 0.5, i + 0.5)
ylim <- range(0, rate$total + se$total, rate$total - se$total)
plot(xlim, ylim, type = "n", xlab = "Segment", ylab = "Rate \u00d7 1000", axes = FALSE,
     xaxs = "i")
usr <- par("usr") # get the user coordinates for later

# put tick marks at multiples of 5 on the x axis; labels at multiples of 10
axis(1, at = i[i %% 5 == 0], labels = FALSE)
axis(1, at = i[i %% 10 == 0], labels = TRUE)

# defaults for the y axis
axis(2)

# label x axis with chapter titles
labels <- data$title
at <- tapply(i, chunks$parent, min) - 0.5

# (adapted from https://www.r-bloggers.com/rotated-axis-labels-in-r-plots/)
text(at, usr[4] + 0.01 * diff(usr[3:4]),
     labels = labels, adj = 0, srt = 45, cex = 0.6, xpd = TRUE)

# put vertical lines at chapter boundaries
abline(v = at, col = "gray")

# frame the plot
box()

# colors for the different emotions, from RColorBrewer::brewer.pal(3, "Set2")
col <- c(total = "#000000", pos = "#FC8D62", neg = "#8DA0CB", ambig = "#66C2A5")

# add a legend on the right hand side
legend(usr[2] + 0.01 * diff(usr[1:2]), usr[3] + 0.8 * diff(usr[3:4]),
       legend = c("Total", "Positive", "Negative", "Ambiguous"),
       title = expression(bold("Emotion")),
       fill = col[c("total", "pos", "neg", "ambig")],
       cex = 0.8, xpd = TRUE)

# for the total rate, put a dashed line at the mean rate
abline(h = mean(rate$total), lty = 2, col = col[["total"]])

# plot each rate type
for (t in c("ambig", "neg", "pos", "total")) {
    r <- rate[[t]]
    s <- se[[t]]
    cl <- col[[t]]

    # add lines and points
    lines(i, r, col = cl)
    points(i, r, col = cl, pch = 16, cex = 0.5)

    # for the total, put standard errors around interesting points
    if (t == "total") {
        # "interesting" defined as rate >2 sd away from mean
        int <- abs((r - mean(r)) / sd(r)) > 2

        segments(i[int], (r - s)[int], i[int], (r + s)[int], col = cl)
        segments((i - .2)[int], (r - s)[int], (i + .2)[int], (r - s)[int], col = cl)
        segments((i - .2)[int], (r + s)[int], (i + .2)[int], (r + s)[int], col = cl)
    }
}
```

There are two segments with particularly high emotion: segment 45 from "The
Wonderful City of Oz", where the Tin Woodman and the Cowardly Lion ask the
Wizard for help, and he compels them to defeat the Wicked Witch of the West;
and segment 64, from "The Discovery of Oz, the Terrible", where Dorothy and
her companions discover that the Wizard of Oz is just a common man.

[gutenberg-oz]: http://www.gutenberg.org/ebooks/55 "The Wonderful Wizard of Oz"
[heaps-law]: https://en.wikipedia.org/wiki/Heaps%27_law "Heap's law"
