<!--
%\VignetteIndexEntry{Introduction to corpus}
%\VignetteEngine{knitr::rmarkdown}
%\VignetteEncoding{UTF-8}
-->

Introduction to corpus
======================

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment = "", fig.path = "corpus-", fig.retina = TRUE)
options(width = 100)
palette(RColorBrewer::brewer.pal(6, "Set1"))
```

Overview
--------

Data preparation
----------------

For a demonstration text we will use L. Frank Baum's *The Wonderful Wizard of
Oz*, available as [Project Gutenberg EBook #55][gutenberg-oz]. We first
download the text and strip off the Project Gutenberg header and footer.

```{r}
url <- "http://www.gutenberg.org/cache/epub/55/pg55.txt"
raw <- readLines(url, encoding = "UTF-8")

# the text starts after the Project Gutenberg header...
start <- grep("^\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK", raw) + 1

# ...end ends at the Project Gutenberg footer.
stop <- grep("^End of Project Gutenberg", raw) - 1

lines <- raw[start:stop]
```

The novel starts with front matter: a title page, table of contents,
introduction, and half title page. Then, a series of chapters follow.
We group the lines by section.
```{r}
# the front matter ends at the half title page
half_title <- grep("^THE WONDERFUL WIZARD OF OZ", lines)

# chapters start with "1.", "2.", etc...
chapter <- grep("^[[:space:]]*[[:digit:]]+\\.", lines)

# ... and appear after the half title page
chapter <- chapter[chapter > half_title]

# get the section texts (including the front matter)
start <- c(1, chapter + 3) # + 3 to skip title and two empty lines
end <- c(chapter - 1, length(lines))
text <- mapply(function(s, e) paste(lines[s:e], collapse = "\n"), start, end)

# discard the front matter
text <- text[-1]

# get the section titles, removing the prefix ("1.", "2.", etc.)
title <- sub("^[[:space:]]*[[:digit:]]+[.][[:space:]]*", "", lines[chapter])
```


Corpus object
-------------

Now that we have obtained our raw data, we put everything together into a
corpus object, constructed via the `corpus` function:

```{r}
data <- corpus(title, text)
rownames(data) <- sprintf("ch%02d", seq_along(chapter))
```

The `corpus` function behaves similarly to the `data.frame` function, but
expects one of the columns to be named `"text"`. Note that we do not need
to specify `stringsAsFactors = FALSE` when creating a corpus object.
As an alternative to using the `corpus` function, we can construct a data
frame using some other method (e.g., `read.csv` or `read_ndjson`) and use
the `as_corpus` function.


A corpus object is just a data frame with a column named "text" of type
`"corpus_text"`. When using the *corpus* library, it is not strictly necessary
to use corpus objects as inputs; most functions will accept with character
vectors and ordinary data frames. Using a corpus object gives better
printing behavior and allows setting a `text_filter` attribute to override
the default text preprocessing.

```{r}
print(data) # better output than printing a data frame, cuts off after 18 rows
print(data, 5) # cuts off after 5 rows
```


Tokenization
------------

Text in *corpus* is represented as a sequence of tokens, each taking a value
in a set of types. We can see the tokens for one or more elements using
the `text_tokens` function:

```{r}
text_tokens(data["ch24",]) # Chapter 24's tokens
```

The default behavior is to normalize tokens by changing the cases of the
letters to lower case. A `text_filter` object controls the rules for
segmentation and normalization. We can inspect the text filter:
```{r}
text_filter(data)
```
We can change the text filter properties:
```{r}
text_filter(data)$map_case <- FALSE
text_filter(data)$drop_punct <- TRUE
text_tokens(data["ch24",])
```

To restore the defaults, set the text filter to `NULL`:
```{r}
text_filter(data) <- NULL
```
In addition to mapping case and quotes (the defaults), I'm going to drop
punctuation.
```{r}
text_filter(data) <- text_filter(drop_punct = TRUE)
```

The tokenizer allows for precise controlling over token dropping and token
stemming. It also allows combining two or more words into a single token as
in the following example:
```{r}
text_tokens("I live in New York City, New York",
            text_filter(combine = c("new york", "new york city")))
```
This example using the optional second argument to `text_tokens` to override
the first argument's default text filter.  Here, instances of "new york" and
"new york city" get replaced by single tokens, with the longest match taking
precedence. See the documentation for `text_tokens` describes the full
tokenization process.


Text statistics
---------------

### Token, type, and sentence counts

The `text_ntoken`, `text_ntype`, and `text_nsentence` functions return the
numbers of tokens, unique types, and sentences, respectively, in a set of
texts. We can use these functions to get an overview of the section lengths
and lexical diversities.

```{r}
stats <- text_stats(data)
print(stats, -1) # print all rows instead of truncating at 18
```

We can see that the last chapter is the shortest, with 74 tokens, 56 unique
types, and 8 sentences. Chapter 12 is the longest.


### Testing Heaps' law

[Heaps' law][heaps-law] says that the logarithm of the number of unique
types is a linear function of the number of tokens. We can test this law
formally with a regression analysis.

In this analysis, we will exclude the last chapter (Chapter 24), because it is
much shorter than the others and has a disproportionate influence on the fit.

```{r}
subset <- row.names(stats) != "ch24"
model <- lm(log(types) ~ log(tokens), stats, subset)
summary(model)
```

We can also inspect the relation visually
```{r heapslaw, fig.width = 12, fig.cap = "Heaps' Law"}
par(mfrow = c(1, 2))
plot(log(types) ~ log(tokens), stats, col = 2, subset = subset)
abline(model, col = 1, lty = 2)

plot(log(stats$tokens[subset]), rstandard(model), col = 2,
     xlab = "log(tokens)")
abline(h = 0, col = 1, lty = 2)

outlier <- abs(rstandard(model)) > 2
text(log(stats$tokens)[subset][outlier], rstandard(model)[outlier],
     row.names(stats)[subset][outlier], cex = 0.75, adj = c(-0.25, 0.5),
     col = 2)
```

The analysis tells us that Heap's law accurately characterizes the lexical
diversity (type-to-token ratio) for the main chapters in *The Wizard of Oz*.
The number of unique types grows roughly as the number of tokens raised to the
power `0.6`.


The one chapter with an unusually low lexical diversity is Chapter 16. This
chapter contains mostly dialogue between Oz and Dorothy's simple-minded
companions (the Scarecrow, Tin Woodman, and Lion).


Term statistics
---------------

### Counts and prevalence

We get term statistics using the `term_stats` function:

```{r}
term_stats(data)
```

This returns a data frame with each row giving the count and support for each
term. The "count" is the total number of occurrences of the term in the
corpus. The "support" is the number of texts containing the term. In the
output above, we can see that "the" is the most common term, appearing 2922
times total in all 24 chapters. The pronoun "he" is the 18th most common
terms, appearing in all but one chapter.


The most common words are function words, commonly known as "stop" words. We
can exclude these terms from the tally using the `subset` argument.

```{r}
term_stats(data, subset = !term %in% stopwords("english"))
```

The character names "dorothy", "toto", and "scarecrow" show up at the top of
the list of the most common terms.


### Higher-order n-grams

Beyond searching for single-type terms, we can also search for multi-type
terms ("n-grams").

```{r}
term_stats(data, ngrams = 5)
```

The `types` argument allows us to request the component types in the result:

```{r}
term_stats(data, ngrams = 3, types = TRUE)
```

Here are the most common 2-, 3-grams starting with "dorothy", where the second
type is not a function word
```{r}
term_stats(data, ngrams = 2:3, types = TRUE,
           subset = type1 == "dorothy" & !type2 %in% stopwords("english"))
```


Searching for terms
-------------------

Now that we have identified common terms, we might be interested in seeing
where they appear. For this, we use the `text_locate` function.

Here are all instances of the term "dorothy looked":

```{r}
text_locate(data, "dorothy looked")
```

Note that we match against the type of the token, not the raw token itself, so
we are able to detect capitalized "Dorothy". This is especially useful when we
wan't to search for a stemmed token. Here are all instances of tokens that
stem to "scream":

```{r}
text_locate(data, "scream", filter = text_filter(stemmer = "english"))
```

If we would like, we can search for multiple phrases at the same time:

```{r}
text_locate(data, c("wicked witch", "toto", "oz"))
```

We can also count term occurrences, test for whether a term appears in a text,
and get the subset of texts containing a term:

```{r}
text_count(data, "the great oz")
text_detect(data, "the great oz")
text_subset(data, "the great oz")
```


Segmenting text
---------------

*Corpus* can split text into blocks of sentences or tokens using the
`text_split` function. By default, this function splits into sentences. Here,
for example, are the last 10 sentences in the book:

```{r}
tail(text_split(data), 10)
```

The result of `text_split` is a data frame, with one row for each segment
identifying the parent text (as a factor), the index of the segment in the
parent text (an integer), and the segment text.


The second argument to `text_split` specifies, the units, "sentences" or
"tokens". The third argument specifies the maximum segment size, defaulting
to one. Each text gets divided into approximately equal-sized segments, with
no segment being larger than the specified size.


Here is an example of splitting two texts into segments of size at most four
tokens.

```{r}
text_split(c("the wonderful wizard of oz", paste(LETTERS, collapse = " ")),
           "tokens", 4)
```


We can combine `text_split` with `text_count` to measure the occurrences rates
for the term "witch" over the course of the novel:

```{r witche-occurences, fig.width = 12, fig.cap = "'witch' Occurences"}
chunks <- text_split(data, "tokens", 500)
size <- text_ntoken(chunks)

unit <- 1000 # rate per 1000 tokens
count <- text_count(chunks, "witch")
rate <-  count / size * unit

i <- seq_along(witch)
plot(i, witch, type = "l", xlab = "Segment",
     ylab = "Rate \u00d7 1000",
     main = paste(dQuote("witch"), "Occurrences"), col = 2)
points(i, witch, pch = 16, cex = 0.5, col = 2)
```

Here, the chunks have varying sizes, so it's important to look at the rates rather
than the raw counts.


Emotional affect
----------------

### Emotion lexicon

Next we will look at usage rates for emotion words from the WordNet Affect
lexicon. As a starting point, we will take the "Positive", "Negative", and
"Ambiguous" emotion terms (excluding the other category, "Neutral").

```{r}
affect <- subset(wnaffect, emotion != "Neutral")
affect$emotion <- droplevels(affect$emotion) # drop the unused "Neutral" level
affect$category <- droplevels(affect$category) # drop unused categories
```

Rather than blindly applying the lexicon, we first check to see what the most
common emotion terms are.
```{r}
term_stats(data, subset = term %in% affect$term)
```

A few terms jump out as unusual: "yellow" is probably for the yellow brick
road, and "wicked" is probably for the wicked witch. When these terms appear,
they probably don't describe an emotional state. We can verify this using the
`text_locate` function, which shows these terms in context.

```{r}
text_locate(data, "yellow")
text_locate(data, "wicked")
```

We can also inspect the first token after each appearance of "wicked":
```{r}
term_stats(text_sub(text_locate(data, "wicked")$after, 1, 1))
```
of the 72 appearances of "wicked", 58 are followed by "witch" or "witches".
Likewise, "yellow" is often referring to the road, or referring to the color
of an object and not an emotion:
```{r}
term_stats(text_sub(text_locate(data, "yellow")$after, 1, 1))
```

The word "heart" is also suspiciously frequent. Here are some occurrences of
that word:
```{r}
text_locate(data, "heart")
```

A central part of the novel's plot is the Tin Woodman's quest for a heart; it
is not surprising that the word "heart" shows up so frequently. Indeed, most
appearances of the word "heart" are within 25 tokens of "woodman":

```{r}
loc <- text_locate(data, "heart")
before <- text_detect(text_sub(loc$before, -25, -1), "woodman")
after <- text_detect(text_sub(loc$after, 1, 25), "woodman")
summary(before | after)
```
"Woodman" appears within 25 tokens of "heart" in in 45 of the 67
contexts where the latter word appears.


All of this analysis shows that we should probably exclude these terms if we
are interested in words that connote emotion in _The Wizard of Oz_:

```{r}
affect <- subset(affect, !term %in% c("heart", "wicked", "yellow"))
```

### Counting emotion words

```{r emotion, fig.width = 12, fig.cap = "Emotion in Oz"}

term_scores <- with(affect, unclass(table(term, emotion)))
term_scores <- term_scores / rowSums(term_scores)

chunks <- text_split(data, "tokens", 500)
n <- text_ntoken(chunks)
x <- term_matrix(chunks, select = rownames(term_scores))
text_scores <- x %*% term_scores

# compute the rates per 1000 tokens
unit <- 1000
rate <- list(pos = text_scores[, "Positive"] / n * unit,
             neg = text_scores[, "Negative"] / n * unit,
             ambig = text_scores[, "Ambiguous"] / n * unit)
rate$total <- rate$pos + rate$neg + rate$ambig

# compute the standard errors
se <- lapply(rate, function(r) sqrt(r * (unit - r) / n))

# set up segment IDs
i <- seq_len(nrow(chunks))

# set the plot margins, with extra space below the plot
par(mar = c(4, 4, 8, 6) + 0.1, las = 1)

# set up the plot coordinates; put labels but no axes
xlim <- range(i - 0.5, i + 0.5)
ylim <- range(0, rate$total + se$total, rate$total - se$total)
plot(xlim, ylim, type = "n", xlab = "Segment", ylab = "Rate \u00d7 1000", axes = FALSE,
     xaxs = "i")
usr <- par("usr") # get the user coordinates for later

# put tick marks at multiples of 5 on the x axis; labels at multiples of 10
axis(1, at = i[i %% 5 == 0], labels = FALSE)
axis(1, at = i[i %% 10 == 0], labels = TRUE)

# defaults for the y axis
axis(2)

# label x axis with chapter titles
labels <- data$title
at <- tapply(i, chunks$parent, min) - 0.5

# (adapted from https://www.r-bloggers.com/rotated-axis-labels-in-r-plots/)
text(at, usr[4] + 0.01 * diff(usr[3:4]),
     labels = labels, adj = 0, srt = 45, cex = 0.6, xpd = TRUE)

# put vertical lines at chapter boundaries
abline(v = at, col = "gray")

# frame the plot
box()

# colors for the different emotions, from RColorBrewer::brewer.pal(3, "Set2")
col <- c(total = "#000000", pos = "#FC8D62", neg = "#8DA0CB", ambig = "#66C2A5")

# add a legend on the right hand side
legend(usr[2] + 0.01 * diff(usr[1:2]), usr[3] + 0.8 * diff(usr[3:4]),
       legend = c("Total", "Positive", "Negative", "Ambiguous"),
       title = expression(bold("Emotion")),
       fill = col[c("total", "pos", "neg", "ambig")],
       cex = 0.8, xpd = TRUE)

# for the total rate, put a dashed line at the mean rate
abline(h = mean(rate$total), lty = 2, col = col[["total"]])

# plot each rate type
for (t in c("ambig", "neg", "pos", "total")) {
    r <- rate[[t]]
    s <- se[[t]]
    cl <- col[[t]]

    # add lines and points
    lines(i, r, col = cl)
    points(i, r, col = cl, pch = 16, cex = 0.5)

    # for the total, put standard errors around interesting points
    if (t == "total") {
        # "interesting" defined as rate >2 sd away from mean
        int <- abs((r - mean(r)) / sd(r)) > 2

        segments(i[int], (r - s)[int], i[int], (r + s)[int], col = cl)
        segments((i - .2)[int], (r - s)[int], (i + .2)[int], (r - s)[int], col = cl)
        segments((i - .2)[int], (r + s)[int], (i + .2)[int], (r + s)[int], col = cl)
    }
}
```

There are two segments with particularly high emotion: segment 45 from "The
Wonderful City of Oz", where the Tin Woodman and the Cowardly Lion ask the
Wizard for help, and he compels them to defeat the Wicked Witch of the West;
and segment 64, from "The Discovery of Oz, the Terrible", where Dorothy and
her companions discover that the Wizard of Oz is just a common man.

[gutenberg-oz]: http://www.gutenberg.org/ebooks/55 "The Wonderful Wizard of Oz"
[heaps-law]: https://en.wikipedia.org/wiki/Heaps%27_law "Heap's law"
