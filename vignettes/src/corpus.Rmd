<!--
%\VignetteIndexEntry{Introduction to corpus}
%\VignetteEngine{knitr::rmarkdown}
%\VignetteEncoding{UTF-8}
-->

Introduction to corpus
======================

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment = "", fig.path = "corpus-", fig.retina = TRUE)
options(width = 120)
palette(RColorBrewer::brewer.pal(6, "Set1"))
```

Overview
--------


Data preparation
----------------

For a demonstration text we will use L. Frank Baum's *The Wonderful Wizard of
Oz*, available as [Project Gutenberg EBook #55][gutenberg-oz]. We first
download the text and strip off the Project Gutenberg header and footer.

```{r}
url <- "http://www.gutenberg.org/cache/epub/55/pg55.txt"
raw <- readLines(url, encoding = "UTF-8")

# the text starts after the Project Gutenberg header...
start <- grep("^\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK", raw) + 1

# ...end ends at the Project Gutenberg footer.
stop <- grep("^End of Project Gutenberg", raw) - 1

lines <- raw[start:stop]
```

The novel starts with front matter: a title page, table of contents,
introduction, and half title page. Then, a series of chapters follow.
We group the lines by section.
```{r}
# the front matter ends at the half title page
half_title <- grep("^THE WONDERFUL WIZARD OF OZ", lines)

# chapters start with "1.", "2.", etc...
chapter <- grep("^[[:space:]]*[[:digit:]]+\\.", lines)

# ... and appear after the half title page
chapter <- chapter[chapter > half_title]

start <- c(1, chapter)
end <- c(chapter - 1, length(lines))

sections <- mapply(function(s, e) paste(lines[s:e], collapse = "\n"),
                   start, end)
names(sections) <- c("front", sprintf("ch%02d", seq_along(chapter)))
```

Finally, we convert the sections from an R character object to a *corpus*
text object, and we store the text in a data frame. This is not strictly
necessary, but it makes downstream processing more convenient. We also
add the `"corpus_frame"` class to the data frame.

```{r}
data <- data.frame(text = as_text(sections))
class(data) <- c("corpus_frame", "data.frame")
```
Note that we do not need to specify `stringsAsFactors = FALSE` for text
objects.  The only affect of the `"corpus_frame"` class is to make printing
nicer:
```{r}
print(data) # cuts off after 18 rows
print(data, 5) # cuts off after 5 rows
```


Tokenization
------------

Text in *corpus* is represented as a sequence of tokens, each taking a value
in a set of types. We can see the tokens for one or more elements using
the `text_tokens` function:

```{r}
text_tokens(data["ch24",]) # Chapter 24's tokens
```

The default behavior is to normalize tokens by changing the cases of the
letters to lower case. A `text_filter` object controls the rules for
segmentation and normalization. We can inspect the text filter:
```{r}
text_filter(data)
```
If the text column is of type `corpus_text` as returned by `as_text`, then
we can change the text filter properties:
```{r}
text_filter(data)$map_case <- FALSE
text_filter(data)$drop_punct <- TRUE
text_tokens(data["ch24",])
```

To restore the defaults, set the text filter to `NULL`:
```{r}
text_filter(data) <- NULL
```
I'm going to drop punctuation and common function words ("stop" words).
```{r}
text_filter(data) <- text_filter(drop_punct = TRUE,
                                 drop = stopwords("english"))
```

The tokenizer allows for precise controlling over token dropping and token
stemming. It also allows combining two or more words into a single token as
in the following example:
```{r}
text_tokens("I live in New York City, New York",
            text_filter(combine = c("new york", "new york city")))
```
This example using the optional second argument to `text_tokens` to override
the first argument's default text filter.  Here, instances of "new york" and
"new york city" get replaced by single tokens, with the longest match taking
precedence. See the documentation for `text_tokens` describes the full
tokenization process.


Text statistics
---------------

The `text_ntoken`, `text_ntype`, and `text_nsentence` functions return the
numbers of tokens, unique types, and sentences, respectively, in a set of
texts. We can use these functions to get an overview of the section lengths
and lexical diversities.

```{r}
stats <- data.frame(tokens = text_ntoken(data),
                    types = text_ntype(data),
                    sentences = text_nsentence(data))
head(stats, n = 10)
```

[Heaps' law][heaps-law] says that the logarithm of the number of unique
types is a linear function of the number of tokens. We can test this law
formally. 

```{r}
# I'm exclude the front matter; the writing mode is different than the
# main chapters. I'm also excluding the last chapter, because it is much
# shorter than the others and has a disproportionate influence.
subset <- !row.names(stats) %in% c("front", "ch24")
model <- lm(log(types) ~ log(tokens), stats, subset)
summary(model)
```

We can also inspect the relation visually
```{r heapslaw, fig.width = 12, fig.cap = "Heaps' Law"}
par(mfrow = c(1, 2))
plot(log(types) ~ log(tokens), stats, col = 2, subset = subset)
abline(model, col = 1, lty = 2)

plot(log(stats$tokens[subset]), rstandard(model), col = 2,
     xlab = "log(tokens)")
abline(h = 0, col = 1, lty = 2)

outlier <- abs(rstandard(model)) > 2
text(log(stats$tokens)[subset][outlier], rstandard(model)[outlier],
     row.names(stats)[subset][outlier], cex = 0.75, adj = c(-0.25, 0.5),
     col = 2)
```

The analysis tells us that Heap's law accurately characterizes the lexical
diversity (type-to-token ratio) for the main chapters in *The Wizard of Oz*.
The number of unique types grows roughly as the number of tokens raised to the
power `0.7`.


The one chapter with an unusually low lexical diversity is Chapter 16. This
chapter contains mostly dialogue between Oz and Dorothy's simple-minded
companions (the Scarecrow, Tin Woodman, and Lion).



[gutenberg-oz]: http://www.gutenberg.org/ebooks/55 "The Wonderful Wizard of Oz"
[heaps-law]: https://en.wikipedia.org/wiki/Heaps%27_law "Heap's law"
