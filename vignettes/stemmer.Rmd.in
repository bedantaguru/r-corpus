---
title: "Stemming Words"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Stemming Words}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment = "", fig.path = "textdata-")
options(width = 95)
set.seed(0)
```

## Snowball stemmer

*Corpus* comes with built-in support for the algorithmic stemmers provided by
the [Snowball Stemming Library][snowball], which supports the following
languages: arabic (ar), danish (da), german (de), english (en), spanish (es),
finnish (fi), french (fr), hungarian (hu), italian (it), dutch (nl), norwegian
(no), portuguese (pt), romanian (ro), russian (ru), swedish (sv), tamil (ta),
and turkish (tr). You can select one of these stemmers using either the full
name of the language of the two-letter country code:

```{r}
text <- "love loving lovingly loved lover lovely love"
text_tokens(text, stemmer = "en") # english stemmer
```

These stemmers are purely algorithmic; they mostly just strip off common
suffixes. 


## Hunspell stemmer

If you want more precise stemming behavior, you can provide a custom stemming
function.  The stemming function should, when given a term as an input, return
the stem of the term as the output.


Here's an example that uses the `hunspell` dictionary to do the stemming.

```{r}
stem_hunspell <- function(term) {
    # look up the term in the dictionary
    stems <- hunspell::hunspell_stem(term)[[1]]

    if (length(stems) == 0) { # if there are no stems, use the original term
        stem <- term
    } else { # if there are multiple stems, use the last one
        stem <- stems[[length(stems)]]
    }

    stem
}

text_tokens(text, stemmer = stem_hunspell)
```


## Dictionary stemmer

One common way to build a stemmer is from a list of (term, stem) pairs. Many
such lists are available at [lexoconista.com][lexiconista]. Here's an example
stemmer for English:

```{r}
# download the list
url <- "http://www.lexiconista.com/Datasets/lemmatization-en.zip"
tmp <- tempfile()
download.file(url, tmp)

# extract the contents
con <- unz(tmp, "lemmatization-en.txt", encoding = "UTF-8")
tab <- read.delim(con, header=FALSE, stringsAsFactors = FALSE)
names(tab) <- c("stem", "term")
```

The first column of this table contains the stem; the second column contains
the raw term:

```{r}
head(tab)
```
We can see that, for example, `"first"` stems to `"1"`.


Here a custom stemming function that uses the list:

```{r}
stem_list <- function(term) {
    i <- match(term, tab$term)
    if (is.na(i)) {
        stem <- term
    } else {
        stem <- tab$stem[[i]]
    }
    stem
}

text_tokens(text, stemmer = stem_list)
```

This pattern is so common that *corpus* provides a convenience function that
will build a stemmer from the input term and stem lists:

```{r}
stem_list2 <- stemmer_make(tab$term, tab$stem)
text_tokens(text, stemmer = stem_list2)
```


## Application: Emotion word usage

Here's how to use a custom stemmer to get counts of emotion word usage. We
will use the text of _The Wizard of Oz_ (Project Gutenberg Work #55)
to demonstrate.

```{r}
data <- gutenberg_corpus(55)
text_filter(data)$stemmer <- with(affect_wordnet,
    stemmer_make(term, interaction(category, emotion), default = NA,
                 ties = "omit"))
```

This stemmer replaces terms by the emotional affect, as listed in the
`affect_wordnet` lexicon. Setting `default = NA` specifies that terms
that are not in the lexicon get dropped. We also specify `ties = "omit"`
so that words listed in multiple categories don't get stemmed.

Here are the (stemmed) term statistics:

```{r}
term_stats(data)
```

We can also get a sample of the instances of the stemmed terms:

```{r}
text_sample(data, "Joy.Positive")
```

[lexiconista]: http://www.lexiconista.com/datasets/lemmatization/ "Lexiconista Lemmatization Lists"
[snowball]: https://snowballstem.org/ "Snowball Stemming Library"
