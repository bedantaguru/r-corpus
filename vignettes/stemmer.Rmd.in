---
title: "Stemming Words"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Stemming Words}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment = "", fig.path = "textdata-")
options(width = 95)
set.seed(0)
```

## Snowball stemmer

*Corpus* comes with built-in support for the algorithmic stemmers provided by
the [Snowball Stemming Library][snowball], which supports the following
languages: arabic (ar), danish (da), german (de), english (en), spanish (es),
finnish (fi), french (fr), hungarian (hu), italian (it), dutch (nl), norwegian
(no), portuguese (pt), romanian (ro), russian (ru), swedish (sv), tamil (ta),
and turkish (tr). You can select one of these stemmers using either the full
name of the language of the two-letter country code:

```{r}
text <- "love loving lovingly loved lover lovely love"
text_tokens(text, stemmer = "en") # english stemmer
```

These stemmers are purely algorithmic; they mostly just strip off common
suffixes. 


## Hunspell stemmer

If you want more precise stemming behavior, you can provide a custom stemming
function.  The stemming function should, when given a term as an input, return
the stem of the term as the output.


Here's an example that uses the `hunspell` dictionary to do the stemming.

```{r}
stem_hunspell <- function(term) {
    # look up the term in the dictionary
    stems <- hunspell::hunspell_stem(term)[[1]]

    if (length(stems) == 0) { # if there are no stems, use the original term
        stem <- term
    } else { # if there are multiple stems, use the last one
        stem <- stems[[length(stems)]]
    }

    stem
}

text_tokens(text, stemmer = stem_hunspell)
```


## Dictionary stemmer

One common way to build a stemmer is from a list of (term, stem) pairs. Many
such lists are available at [lexoconista.com][lexiconista]. Here's an example
stemmer for English:

```{r}
# download the list
url <- "http://www.lexiconista.com/Datasets/lemmatization-en.zip"
tmp <- tempfile()
download.file(url, tmp)

# extract the contents
con <- unz(tmp, "lemmatization-en.txt", encoding = "UTF-8")
tab <- read.delim(con, header=FALSE, stringsAsFactors = FALSE)
names(tab) <- c("stem", "term")
```

The first column of this table contains the stem; the second column contains
the raw term:

```{r}
head(tab)
```
We can see that, for example, `"first"` stems to `"1"`.


Here a custom stemming function that uses the list:

```{r}
stem_list <- function(term) {
    i <- match(term, tab$term)
    if (is.na(i)) {
        stem <- term
    } else {
        stem <- tab$stem[[i]]
    }
    stem
}

text_tokens(text, stemmer = stem_list)
```

This pattern is so common that *corpus* provides a convenience function that
will build a stemmer from the input term and stem lists:

```{r}
stem_list2 <- new_stemmer(tab$term, tab$stem)
text_tokens(text, stemmer = stem_list2)
```


## Application: Emotion word usage

Here's how to use a custom stemmer to get counts of emotion word usage. We
will use the text of _The Wizard of Oz_ (Project Gutenberg Work #55)
to demonstrate.

```{r}
data <- gutenberg_corpus(55, verbose = FALSE)
text_filter(data)$stemmer <-
    with(affect_wordnet,
        new_stemmer(term, interaction(category, emotion),
                    default = NA, duplicates = "omit"))
```

This stemmer replaces terms by the emotional affect, as listed in the
`affect_wordnet` lexicon. Setting `default = NA` specifies that terms
that are not in the lexicon get dropped. We also specify
`duplicates = "omit"` so that words listed in multiple categories
get replaced with the default (i.e., they get dropped).

Here are the (stemmed) term statistics:

```{r}
print(term_stats(data), -1)
```

We can also get a sample of the instances of the stemmed terms:

```{r}
text_sample(data, "Joy.Positive")
```


## Application: Spell-corrected tokens

Suppose we want to analyze texts with spelling errors, using our best
guess of the intended word rather than the literal spelling in the text. We
can do this by using a stemmer that tries to correct spelling errors in the
tokens:

```{r}
stem_spellcorrect <- function(term) {
    # if the term is spelled correctly, leave it as-is
    if (hunspell::hunspell_check(term)) {
        return(term)
    }

    suggestions <- hunspell::hunspell_suggest(term)[[1]]

    # if hunspell found a suggestion, use the first one
    if (length(suggestions) > 1) {
        suggestions[[1]]
    } else {
        # otherwise, use the original term
        term
    }
}
```

Here's an example use of the stemmer:
```{r}
text <- "spell checkers are not neccessairy for langauge ninja's"
text_tokens(text, stemmer = stem_spellcorrect)
```


## Efficiency considerations

When you stem a text, the result gets cached, so you never have to stem the
same type twice. If the input is a `corpus_text` object, these cached values
are shared across multiple tokenizations.

Here's a stemmer that prepends "bunny" to every word, keeping track of how
many times it gets called:

```{r}
nbunny <- 0
stem_bunny <- function(term) {
    nbunny <<- nbunny + 1
    paste("bunny", term)
}
```

We will set this as the stemmer for a corpus (a data frame with a "text"
column of type `corpus_text`):
```{r}
corpus <- as_corpus_frame(federalist)
text_filter(corpus)$stemmer <- stem_bunny
```

Here's how long it takes to tokenize the text once and compute the term
statistics:
```{r}
system.time(stats <- term_stats(corpus))
```

Here's how many times the stemmer got called:
```{r}
print(nbunny)
```

We will now set the bunny count to zero and run the same computation:

```{r}
nbunny <- 0
system.time(stats2 <- term_stats(corpus))
```

It took us half the time. How many additional calls to the stemmer were there?

```{r}
print(nbunny)
```

No additional calls. We used the cached stems from the first call to
`term_stats`. We can verify that the results were the same both times.

```{r}
identical(stats, stats2)
```

Just for fun, here are the results:

```{r}
stats
```

[lexiconista]: http://www.lexiconista.com/datasets/lemmatization/ "Lexiconista Lemmatization Lists"
[snowball]: https://snowballstem.org/ "Snowball Stemming Library"
